# gpncup_2023_attempt
Мое решение GPN 2023, направление "Data science"
_Набиуллин Иван_

_tg __: @__ xenopupel_

Здравствуйте!

1. **Изучение зависимостей**

Первое, что я делал после предобработки данных, изучал зависимости переменных друг от друга (в каждой комбинации город - товар).

![Image alt](https://github.com/xenopupel/gpncup_2023_attempt/tree/main/images/1.png)

_Рисунок 1 Анор Лондо – Эстус_

- Цена, себестоимость и средняя цена конкурентов сильно коррелируют друг с другом.
- Объемы продаж склонны зависеть от погоды
- Цена не зависит от объема продаж напрямую

1. **Метрики**

Использовал **корень** из значения, полученного кросс-валидацией по временному ряду с MSE. Корень, чтобы ошибка была в тех же единицах, в которых измеряется цена.

Так же в процессе работы пригодились RMSE и MAPE при разбиении всей выборки на train, valid и test. Так я мог строить графики по train и по valid и наблюдать за ошибками тщательнее.

1. **Выбор модели**

Первой идеей было просто построить модель регрессии через случайный лес, чтобы было с чем сравнивать. Модель дала плохие результаты с RMSE = 9,42 (эту часть решил не вставлять в .ipynb файл).

Я посмотрел на графики по всему, чему можно было, и принял решение строить модель линейной регрессии, однако тут возникла проблема. Линейная регрессия не учитывает временную зависимость. Чтобы решить эту проблему я решил использовать Lagfeatures.

![Image alt](https://github.com/xenopupel/gpncup_2023_attempt/tree/main/images/2.png)

Сдвигать можно не один признак, и не на одну дату, поэтому на этом этапе наступил момент подбора параметров для лучших результатов.

Лучшее решение (без регуляризации), которое я нашел это сдвинуть 10 раз на [90:100] дат соответственно следующие признаки:

- Цена
- Скользящее среднее с окном в 50 (сгенерировал ранее)
- Скользящее среднее себестоимости с окном в 25

Скользящее среднее себестоимости, а не сама себестоимость, потому что данные содержали много пропусков. Я восстановил их, используя линейную интерполяцию, но, в любом случае, лучше сгладить.

![Image alt](https://github.com/xenopupel/gpncup_2023_attempt/tree/main/images/3.png)

_Рисунок 2 – RMSE и MAPE на **валидационной** выборке. Кросс валидация учитывает все._

Так же пробовал генерировать новые фичи с помощью библиотеки tsfresh, которая автоматически достает признаки из временных рядов. Результаты получились интересными, думаю она плохо работает вместе с Lag фичами, поэтому результат слегка ухудшился.

![Image alt](https://github.com/xenopupel/gpncup_2023_attempt/tree/main/images/4.png)

_Рисунок 3- Lagfeatures + tsfresh_

Результат с Lag фичей по цене, но теперь только в одном количестве. Один сдвиг на 90 дней. + tsfresh:

![Image alt](https://github.com/xenopupel/gpncup_2023_attempt/tree/main/images/5.png)

_Рисунок 4 -1 Lag + tsfresh_

CV лучше, но две другие ошибки хуже, и графики мне не понравились, поэтому я решил отказаться от этой комбинации параметров.

Пробовал использовать L1 и L2 регуляризации. Про L1 сразу было понятно, что все коэффициенты у лаг фичей превратятся в 0. А вот L2 было интересно посмотреть. Alpha = 50, 10 лаг фич по трем признакам, как и до этого.

![Image alt](https://github.com/xenopupel/gpncup_2023_attempt/tree/main/images/6.png)

_Рисунок 5 - Все lag + L2_

![Image alt](https://github.com/xenopupel/gpncup_2023_attempt/tree/main/images/7.png)

_Рисунок 6 - train_

![Image alt](https://github.com/xenopupel/gpncup_2023_attempt/tree/main/images/8.png)

_Рисунок 7 - valid_

Мне понравились графики, и CV меньше, поэтому остался на этом варианте несмотря на возросшие RMSE и MAPE.

Стоит заметить, что себестоимость можно было заменить средней ценой конкурентов, т.к. у них большая корреляция между друг другом (рис.1). Вместе их использовать нельзя, появится мультиколлинеарность (_я пробовал поставить их вместе и использовать_ _L__2 регуляризацию для уменьшения мультиколлинеарности, вышло не очень_).

Подумал, что цена логичнее описывается себестоимостью, чем ценой конкурентов (_конкуренты вполне могут зависеть от нашей себестоимости в данном городе на данный продукт, вдруг у нас похожие себестоимости_).

1. **Предсказание цены соперников**

Всё бы хорошо, но есть правило о 20%, постарался придумать как его соблюсти.

Если мы посмотрим на графики средних цен конкурентов, то сразу увидим, что они растут достаточно линейно, поэтому я подумал, что для предсказания нам хватит регрессии по индексам дат.

![Image alt](https://github.com/xenopupel/gpncup_2023_attempt/tree/main/images/9.png)

![Image alt](https://github.com/xenopupel/gpncup_2023_attempt/tree/main/images/10.png)

![Image alt](https://github.com/xenopupel/gpncup_2023_attempt/tree/main/images/11.png)

![Image alt](https://github.com/xenopupel/gpncup_2023_attempt/tree/main/images/12.png)

После предсказаний цен соперников мы можем поэлементно сравнить каждое предсказание нашей цены с конкурентами, и если наше превышает конкурента больше чем на 20%, то наша цена принимает значение цены конкурента\*1.2.

Вот пример проблемного предсказания (их, на удивление было всего 2, в остальных случаях предсказания не вылезают за примерные 20%)

![Image alt](https://github.com/xenopupel/gpncup_2023_attempt/tree/main/images/13.png)

После этого применил функцию, которая привела все предсказания наших цен к ступенчатому виду, а также округлила до 2 серебряных (соблюдаем правила).

1. **Что не успел реализовать.**

Я не успел предсказать количество, а его, скорее всего, получится достаточно точно предсказать, т.к. это в большинстве своем красивые временные ряды, я с общими максимумами, минимумами. У них почти везде одинаковая амплитуда, а еще, на количество продаж влияет погода (они скоррелированы положительно, я не стал предполагать, что от объемов продаж зависит погода).
 Само собой, если бы я просто предсказал объемы продаж, это бы ничего не дало, нужно знать и себестоимость, но по тем данным с огромным количеством пропусков было бы тяжело что-то спрогнозировать. В любом случае, можно предсказать, предварительно сгладив кривую. И после этого можно было бы посчитать прибыль, но я не успеваю.
